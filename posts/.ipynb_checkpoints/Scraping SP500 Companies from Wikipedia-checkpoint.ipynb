{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping S&P 500 Components from Wikipedia \n",
    "\n",
    "### What we're going to do: \n",
    "1. Scrape wikipedia for lists of: \n",
    "    - Current companies in the S&P 500 \n",
    "    - Historical changes to the S&P 500 \n",
    "2. Write a function to generate the list of companies in the index as of a given date \n",
    "<!-- TEASER_END --> \n",
    "\n",
    "### Background \n",
    "Getting the current list of companies in the S&P 500 is pretty easy so we're gonna tackle that first. Reconstructing the index historically isn't so easy. Since the index is regularly rebalanced, we need a list of all the companies added and removed from the index and the date the change occurred. \n",
    "\n",
    "Wikipedia is nice enough to make this data available, but as we'll see shortly, the format of the table of company changes is a little tricky and requires some web scraping gymnastics to get it into a useable format for analysis. Let's get to it. \n",
    "\n",
    "**Attribution:** Two of the big problems I ran into were solved by a fellow named Andy Roche and he was nice enough to write a blog post with his approach an code. [Here's his post](https://roche.io/2016/05/scrape-wikipedia-with-python) so be sure to check that out for a more thorough approach to wikipedia tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries \n",
    "First up, import libraries, get the site HTML with request.get(), then extract the tables for further cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime \n",
    "import re \n",
    "\n",
    "# wikipedia page with our target tables and the initial web request \n",
    "WIKI_URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "req = requests.get(WIKI_URL)\n",
    "req.raise_for_status()\n",
    "\n",
    "# here we search for all the tables on the web page and get them into a \n",
    "# beautiful soup result set  \n",
    "soup = BeautifulSoup(req.content, 'lxml')\n",
    "table_classes = {\"class\": [\"sortable\", \"plainrowheaders\"]}\n",
    "wikitables = soup.findAll(\"table\", table_classes)\n",
    "type(wikitables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the table of current companies \n",
    "We're interested in the first two tables on the page web page. The first table is a pretty clean HTML table that lists all the companies currently in the S&P 500. We're going to traverse the table, clean thing up a bit, then store the results in a list for use later. Note the regular expression to strip out the wikipedia footnotes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Symbol',\n",
       "  'Security',\n",
       "  'SEC filings',\n",
       "  'GICS Sector',\n",
       "  'GICS Sub Industry',\n",
       "  'Headquarters Location',\n",
       "  'Date first added',\n",
       "  'CIK',\n",
       "  'Founded'],\n",
       " ['MMM',\n",
       "  '3M Company',\n",
       "  'reports',\n",
       "  'Industrials',\n",
       "  'Industrial Conglomerates',\n",
       "  'St. Paul, Minnesota',\n",
       "  '',\n",
       "  '0000066740',\n",
       "  '1902'],\n",
       " ['ABT',\n",
       "  'Abbott Laboratories',\n",
       "  'reports',\n",
       "  'Health Care',\n",
       "  'Health Care Equipment',\n",
       "  'North Chicago, Illinois',\n",
       "  '1964-03-31',\n",
       "  '0000001800',\n",
       "  '1888']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = wikitables[0].find_all(\"tr\")\n",
    "\n",
    "#parse data from table by extracting each table row (\"tr\" tags) \n",
    "current_companies_list = []   \n",
    "for tr in rows:\n",
    "    if rows.index(tr) == 0: \n",
    "        row_cells = [th.getText().strip() for th in tr.find_all('th') \n",
    "                        if th.getText().strip() != '']  \n",
    "    else: \n",
    "        row_cells = (([tr.find('th').getText()] if tr.find('th') else []) \n",
    "                        + [td.getText().strip() for td in tr.find_all('td')])\n",
    "    if len(row_cells) > 1: \n",
    "        # strip out brackets from reference links \n",
    "        for i, element in enumerate(row_cells): \n",
    "            if element.find('[') != -1: \n",
    "                row_cells[i] = re.sub(\"[\\[].*?[\\]]\", \"\", element)\n",
    "        current_companies_list += [row_cells]\n",
    "        \n",
    "current_companies_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a clean list where the first element is a list of headers and each element after it relates to a single company. We're mostly interested in the ticker symbol for each company but it doesn't hurt too keep the additional reference info for now. Not too shabby! \n",
    "\n",
    "**Next comes the hard part:** the history of changes to the index components. The second table has the data we need but it also has lots of rows where a single data element spans multiple rows. This isn't good for data analysis so here's what we have to do: \n",
    "- The first column is the date a change occurred so we'll write a helper function to check if it's a date\n",
    "    - If we find a date, we'll hold it in a temporary variable and repeat it for each row it spans in the original HTML table  \n",
    "- Next we'll clean the data so we wind up with one list element per change, in the following format: \n",
    "    - [Date, Added, Removed, Reason]\n",
    "- We also need to explicitly keep blank cells (sometimes companies are added and none are removed or vice versa) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Date', 'Added', 'Removed', 'Reason'],\n",
       " ['', 'Ticker'],\n",
       " ['January 2, 2019',\n",
       "  'FRC',\n",
       "  'First Republic Bank',\n",
       "  'SCG',\n",
       "  'SCANA',\n",
       "  'Dominion Energy acquiring SCANA Corporation'],\n",
       " ['December 24, 2018',\n",
       "  'CE',\n",
       "  'Celanese Corp.',\n",
       "  'ESRX',\n",
       "  'Express Scripts',\n",
       "  'S&P 500 constituent Cigna (NYSE: CI) acquired ESRX'],\n",
       " ['December 3, 2018',\n",
       "  'LW',\n",
       "  'Lamb Weston Holdings Inc',\n",
       "  'COL',\n",
       "  'Rockwell Collins Inc',\n",
       "  'UTX acquires COL '],\n",
       " ['December 3, 2018',\n",
       "  'MXIM',\n",
       "  'Maxim Integrated Products Inc',\n",
       "  'AET',\n",
       "  'Aetna Inc',\n",
       "  'CVS acquires Aetna']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get table of changes into bs4 result set \n",
    "row_chgs = wikitables[1].find_all(\"tr\")\n",
    "\n",
    "#function to check if first element is a date \n",
    "def date_check(date_text): \n",
    "    try: \n",
    "        datetime.datetime.strptime(date_text, '%B %d, %Y')\n",
    "        return True \n",
    "    except ValueError: \n",
    "        return False \n",
    "\n",
    "# parse data as is\n",
    "company_changes_list, date_holder, reason_holder = [], '', ''\n",
    "for tr in row_chgs:\n",
    "    if row_chgs.index(tr) == 0: \n",
    "        row_cells = [th.getText().strip() for th in tr.find_all('th') \n",
    "                        if th.getText().strip() != '']  \n",
    "    else: \n",
    "        row_cells = (([tr.find('th').getText()] if tr.find('th') else []) \n",
    "                        + [td.getText().strip() for td in tr.find_all('td')])\n",
    "        # check if element is a date \n",
    "        if date_check(row_cells[0]): \n",
    "            date_holder = row_cells[0]\n",
    "            reason_holder = row_cells[-1]\n",
    "        else: \n",
    "            row_cells.insert(0, date_holder)\n",
    "            if len(row_cells) == 5: \n",
    "                row_cells.append(reason_holder) \n",
    "    if len(row_cells) > 1: \n",
    "        # strip out brackets from reference links \n",
    "        if len(row_cells) == 6: \n",
    "            row_cells[5] = re.sub(\"[\\[].*?[\\]]\", \"\", row_cells[5])\n",
    "        company_changes_list += [row_cells]\n",
    "\n",
    "company_changes_list[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok we're good to go! Notice the second element of the list has a junk entry in it since the table headers aren't consistent - such is life when scraping data from the web! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Date', 'Added', 'Removed', 'Reason'],\n",
       " ['January 2, 2019',\n",
       "  'FRC',\n",
       "  'First Republic Bank',\n",
       "  'SCG',\n",
       "  'SCANA',\n",
       "  'Dominion Energy acquiring SCANA Corporation'],\n",
       " ['December 24, 2018',\n",
       "  'CE',\n",
       "  'Celanese Corp.',\n",
       "  'ESRX',\n",
       "  'Express Scripts',\n",
       "  'S&P 500 constituent Cigna (NYSE: CI) acquired ESRX'],\n",
       " ['December 3, 2018',\n",
       "  'LW',\n",
       "  'Lamb Weston Holdings Inc',\n",
       "  'COL',\n",
       "  'Rockwell Collins Inc',\n",
       "  'UTX acquires COL '],\n",
       " ['December 3, 2018',\n",
       "  'MXIM',\n",
       "  'Maxim Integrated Products Inc',\n",
       "  'AET',\n",
       "  'Aetna Inc',\n",
       "  'CVS acquires Aetna'],\n",
       " ['December 3, 2018',\n",
       "  'FANG',\n",
       "  'Diamondback Energy Inc',\n",
       "  'SRCL',\n",
       "  'Stericycle Inc',\n",
       "  'Market Capitalization change']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final bit of cleaning - delete the last bit of junk HTML in the second list element \n",
    "del company_changes_list[1]\n",
    "\n",
    "company_changes_list[:6]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nikola": {
   "date": "2019-01-18 19:52:05 UTC",
   "slug": "SP500-components-webscrape",
   "tags": "python, web scraping, stock market data",
   "title": "Scraping S&P 500 Components from Wikipedia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
