{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping S&P 500 Components from Wikipedia \n",
    "\n",
    "### What we're going to do: \n",
    "1. Scrape wikipedia for lists of: \n",
    "    - Current companies in the S&P 500 \n",
    "    - Historical changes to the S&P 500 \n",
    "2. Write a function to generate the list of companies in the index as of a given date \n",
    "<!-- TEASER_END --> \n",
    "\n",
    "### Background \n",
    "Getting the current list of companies in the S&P 500 is pretty easy so we're gonna tackle that first. Reconstructing the index historically isn't so easy. Since the index is regularly rebalanced, we need a list of all the companies added and removed from the index and the date the change occurred. \n",
    "\n",
    "Wikipedia is nice enough to make this data available and [here's the web page](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies) if you want to see what it looks like. As we'll see shortly, the format of the table of company changes is a little tricky and requires some coding gymnastics to get it into a useable format for analysis. Let's get to it. \n",
    "\n",
    "**Attribution:** Two of the big problems I ran into (multiple tables on a page and table data elements that span multiple rows) were solved by a fellow named Andy Roche and he was nice enough to share his approach and code in a blog post. [Here's his post](https://roche.io/2016/05/scrape-wikipedia-with-python) so be sure to check that out for a more thorough approach to wikipedia tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries \n",
    "First up, import libraries, get the site HTML with `request.get()`, then extract the tables from the BeautifulSoup ResultSet object for further cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime \n",
    "import re \n",
    "\n",
    "# wikipedia page with our target tables and the initial web request \n",
    "WIKI_URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "req = requests.get(WIKI_URL)\n",
    "req.raise_for_status()\n",
    "data_retrieval_date = datetime.datetime.today() \n",
    "\n",
    "# here we search for all the tables on the web page and get them into a \n",
    "# beautiful soup result set  \n",
    "soup = BeautifulSoup(req.content, 'lxml')\n",
    "table_classes = {\"class\": [\"sortable\", \"plainrowheaders\"]}\n",
    "wikitables = soup.findAll(\"table\", table_classes)\n",
    "type(wikitables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the table of current companies \n",
    "We're interested in the first two tables on the page web page. The first table is a pretty clean HTML table that lists all the companies currently in the S&P 500. We're going to traverse the table, clean thing up a bit, then store the results in a list for use later. Note the regular expression toward the end of the code block to strip out the wikipedia footnotes contained in brackets ([]). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Symbol',\n",
       "  'Security',\n",
       "  'SEC filings',\n",
       "  'GICS Sector',\n",
       "  'GICS Sub Industry',\n",
       "  'Headquarters Location',\n",
       "  'Date first added',\n",
       "  'CIK',\n",
       "  'Founded'],\n",
       " ['MMM',\n",
       "  '3M Company',\n",
       "  'reports',\n",
       "  'Industrials',\n",
       "  'Industrial Conglomerates',\n",
       "  'St. Paul, Minnesota',\n",
       "  '',\n",
       "  '0000066740',\n",
       "  '1902']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = wikitables[0].find_all(\"tr\")\n",
    "\n",
    "#parse data from table by extracting each table row (\"tr\" tags) \n",
    "current_companies_list = []   \n",
    "for tr in rows:\n",
    "    if rows.index(tr) == 0: \n",
    "        row_cells = [th.getText().strip() for th in tr.find_all('th') \n",
    "                        if th.getText().strip() != '']  \n",
    "    else: \n",
    "        row_cells = (([tr.find('th').getText()] if tr.find('th') else []) \n",
    "                        + [td.getText().strip() for td in tr.find_all('td')])\n",
    "    if len(row_cells) > 1: \n",
    "        # strip out brackets from reference links \n",
    "        for i, element in enumerate(row_cells): \n",
    "            if element.find('[') != -1: \n",
    "                row_cells[i] = re.sub(\"[\\[].*?[\\]]\", \"\", element)\n",
    "        current_companies_list += [row_cells]\n",
    "        \n",
    "current_companies_list[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Progress!** Now we have a clean list of lists where the first element is a list of headers and each element after it relates to a single company. We're mostly interested in the ticker symbol for each company but it doesn't hurt to keep the additional reference info for now. Not too shabby! \n",
    "\n",
    "### Next comes the hard part: parsing the table of changes  \n",
    "Parsing data from the table of index component changes is a little more difficult. Some cells are blank, and some of the date values span multiple rows when more than one company change occurred on the same date. In order to get the data into a better format for analysis, here's what we need to do: \n",
    "- The first column is the date a change occurred so we'll write a helper function to check if it's a date\n",
    "    - If we find a date, we'll hold it in a temporary variable and repeat it for each row it spans in the original HTML table  \n",
    "- Next we'll clean the data so we wind up with one list element per change, accounting for the dates that span multiple rows (repeating the date when necessary) \n",
    "- We also need to explicitly keep blank cells in the added/removed columns to correctly handle the times when companies are added and none are removed or vice versa\n",
    "- Each list element will be in the following format:\n",
    "    - [Date, Added (ticker and company name), Removed (ticker and company name), Reason]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Date', 'Added', 'Removed', 'Reason'],\n",
       " ['', 'Ticker'],\n",
       " ['January 18, 2019',\n",
       "  'TFX',\n",
       "  'Teleflex Inc',\n",
       "  'PCG',\n",
       "  'PG&E Corp',\n",
       "  'PCG filing for bankrupcy'],\n",
       " ['January 2, 2019',\n",
       "  'FRC',\n",
       "  'First Republic Bank',\n",
       "  'SCG',\n",
       "  'SCANA',\n",
       "  'Dominion Energy acquiring SCANA Corporation'],\n",
       " ['December 24, 2018',\n",
       "  'CE',\n",
       "  'Celanese Corp.',\n",
       "  'ESRX',\n",
       "  'Express Scripts',\n",
       "  'S&P 500 constituent Cigna (NYSE: CI) acquired ESRX'],\n",
       " ['December 3, 2018',\n",
       "  'LW',\n",
       "  'Lamb Weston Holdings Inc',\n",
       "  'COL',\n",
       "  'Rockwell Collins Inc',\n",
       "  'UTX acquires COL ']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get table of changes into bs4 result set \n",
    "row_chgs = wikitables[1].find_all(\"tr\")\n",
    "\n",
    "#function to check if first element is a date \n",
    "def date_check(date_text): \n",
    "    try: \n",
    "        datetime.datetime.strptime(date_text, '%B %d, %Y')\n",
    "        return True \n",
    "    except ValueError: \n",
    "        return False \n",
    "\n",
    "# parse data as is\n",
    "company_changes_list, date_holder, reason_holder = [], '', ''\n",
    "for tr in row_chgs:\n",
    "    if row_chgs.index(tr) == 0: \n",
    "        row_cells = [th.getText().strip() for th in tr.find_all('th') \n",
    "                        if th.getText().strip() != '']  \n",
    "    else: \n",
    "        row_cells = (([tr.find('th').getText()] if tr.find('th') else []) \n",
    "                        + [td.getText().strip() for td in tr.find_all('td')])\n",
    "        # check if element is a date \n",
    "        if date_check(row_cells[0]): \n",
    "            date_holder = row_cells[0]\n",
    "            reason_holder = row_cells[-1]\n",
    "        else: \n",
    "            row_cells.insert(0, date_holder)\n",
    "            if len(row_cells) == 5: \n",
    "                row_cells.append(reason_holder) \n",
    "    if len(row_cells) > 1: \n",
    "        # strip out brackets from reference links \n",
    "        if len(row_cells) == 6: \n",
    "            row_cells[5] = re.sub(\"[\\[].*?[\\]]\", \"\", row_cells[5])\n",
    "        company_changes_list += [row_cells]\n",
    "\n",
    "company_changes_list[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Almost there!** Notice the second element of the list has a junk entry in it - this is due to the odd layout of the HTML table header but such is life when scraping data from the web! Here are the final housekeeping items for this list: \n",
    "- Delete the first 2 elements \n",
    "    - We mentioned the junk HTML element but the header row also has to go since we'll be sorting the list for use later in the function \n",
    "- Convert the first element of each list to a datetime object \n",
    "- Sort the entire list by the date elements in reverse order (most recent changes first) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[datetime.datetime(2019, 1, 18, 0, 0),\n",
       "  'TFX',\n",
       "  'Teleflex Inc',\n",
       "  'PCG',\n",
       "  'PG&E Corp',\n",
       "  'PCG filing for bankrupcy'],\n",
       " [datetime.datetime(2019, 1, 2, 0, 0),\n",
       "  'FRC',\n",
       "  'First Republic Bank',\n",
       "  'SCG',\n",
       "  'SCANA',\n",
       "  'Dominion Energy acquiring SCANA Corporation'],\n",
       " [datetime.datetime(2018, 12, 24, 0, 0),\n",
       "  'CE',\n",
       "  'Celanese Corp.',\n",
       "  'ESRX',\n",
       "  'Express Scripts',\n",
       "  'S&P 500 constituent Cigna (NYSE: CI) acquired ESRX']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del company_changes_list[:2]\n",
    "for i in company_changes_list: \n",
    "    i[0] = datetime.datetime.strptime(i[0], '%B %d, %Y') \n",
    "\n",
    "company_changes_list.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "company_changes_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that builds active company ticker lists as of a given date \n",
    "Now we'll write a function that takes a date as an argument and uses the lists we just built to return the companies that were active in the S&P 500 as of the input date. We'll also specify a function argument 'output_type' to determine the format of the returned list. A list object is easier to work with if you want do some slicing or iterating and a string is useful if you're going to feed it directly into a URL string to retrieve data from an API (more on how to do this in later posts). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A,AAL,AAP,AAPL,ABBV,ABC,ABT,ACN,ADBE,ADI,ADM,ADP,ADS,ADSK,AEE,AEP,AES,AET,AFL,AGN,AIG,AIV,AIZ,AJG,AKAM,ALB,ALK,ALL,ALLE,ALXN,AMAT,AME,AMG,AMGN,AMP,AMT,AMZN,AN,ANDV,ANTM,AON,APA,APC,APD,APH,APTV,ARNC,ATVI,AVB,AVGO,AVY,AWK,AXP,AYI,AZO,BA,BAC,BAX,BBBY,BBT,BBY,BCR,BDX,BEN,BF-B,BHGE,BIIB,BK,BKNG,BLK,BLL,BMY,BRK-B,BSX,BWA,BXP,C,CA,CAG,CAH,CAT,CB,CBRE,CBS,CCI,CCL,CELG,CERN,CF,CFG,CHD,CHK,CHRW,CHTR,CI,CINF,CL,CLX,CMA,CMCSA,CME,CMG,CMI,CMS,CNC,CNP,COF,COG,COL,COO,COP,COST,COTY,CPB,CPRI,CRM,CSCO,CSRA,CSX,CTAS,CTL,CTSH,CTXS,CVS,CVX,CXO,D,DAL,DD,DE,DFS,DG,DGX,DHI,DHR,DIS,DISCA,DISCK,DLR,DLTR,DNB,DOV,DPS,DRI,DTE,DUK,DVA,DVN,DWDP,EA,EBAY,ECL,ED,EFX,EIX,EL,EMN,EMR,ENDP,EOG,EQIX,EQR,EQT,ES,ESRX,ESS,ETFC,ETN,ETR,EVHC,EW,EXC,EXPD,EXPE,EXR,F,FAST,FB,FBHS,FCX,FDX,FE,FFIV,FIS,FISV,FITB,FL,FLIR,FLR,FLS,FMC,FOX,FOXA,FRT,FSLR,FTI,FTR,FTV,GD,GE,GGP,GILD,GIS,GLW,GM,GOOG,GOOGL,GPC,GPN,GPS,GRMN,GS,GT,GWW,HAL,HAR,HAS,HBAN,HBI,HCA,HCP,HD,HES,HIG,HOG,HOLX,HON,HP,HPE,HPQ,HRB,HRL,HRS,HSIC,HST,HSY,HUM,IBM,ICE,IFF,ILMN,INTC,INTU,IP,IPG,IQV,IR,IRM,ISRG,ITW,IVZ,JBHT,JCI,JEC,JEF,JNJ,JNPR,JPM,JWN,K,KEY,KHC,KIM,KLAC,KMB,KMI,KMX,KO,KR,KSS,KSU,L,LB,LEG,LEN,LH,LIN,LKQ,LLL,LLTC,LLY,LMT,LNC,LNT,LOW,LRCX,LUV,LVLT,LYB,M,MA,MAA,MAC,MAR,MAS,MAT,MCD,MCHP,MCK,MCO,MDLZ,MDT,MET,MHK,MJN,MKC,MLM,MMC,MMM,MNK,MNST,MO,MON,MOS,MPC,MRK,MRO,MS,MSFT,MSI,MTB,MTD,MU,MUR,MYL,NAVI,NBL,NDAQ,NEE,NEM,NFLX,NFX,NI,NKE,NLSN,NOC,NOV,NRG,NSC,NTAP,NTRS,NUE,NVDA,NWL,NWS,NWSA,O,OKE,OMC,ORCL,ORLY,OXY,PAYX,PBCT,PBI,PCAR,PCG,PDCO,PEG,PEP,PFE,PFG,PG,PGR,PH,PHM,PKI,PLD,PM,PNC,PNR,PNW,PPG,PPL,PRGO,PRU,PSA,PSX,PVH,PWR,PXD,PYPL,QCOM,QRVO,R,RAI,RCL,REGN,RF,RHI,RHT,RIG,RL,ROK,ROP,ROST,RRC,RSG,RTN,SBUX,SCG,SCHW,SE,SEE,SHW,SIG,SJM,SLB,SLG,SNA,SNI,SO,SPG,SPGI,SPLS,SRCL,SRE,STI,STJ,STT,STX,STZ,SWK,SWKS,SWN,SYF,SYK,SYMC,SYY,T,TAP,TDC,TDG,TEL,TGNA,TGT,TIF,TJX,TMK,TMO,TPR,TRIP,TROW,TRV,TSCO,TSN,TSS,TWX,TXN,TXT,UA,UAA,UAL,UDR,UHS,ULTA,UNH,UNM,UNP,UPS,URBN,URI,USB,UTX,V,VAR,VFC,VIAB,VLO,VMC,VNO,VRSK,VRSN,VRTX,VTR,VZ,WAT,WBA,WDC,WEC,WELL,WFC,WFM,WHR,WLTW,WM,WMB,WMT,WRK,WU,WY,WYN,WYNN,XEC,XEL,XL,XLNX,XOM,XRAY,XRX,XYL,YHOO,YUM,ZBH,ZION,ZTS'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_date must be in 'YYYY-MM-DD' format \n",
    "def active_ticker_list_builder(input_date, output_type='string'):\n",
    "    # convert input date to datetime object (must be in 'YYYY-MM-DD' format)\n",
    "    input_date_dt = datetime.datetime.strptime(input_date, '%Y-%m-%d')\n",
    "    # extract tickers only from current_companies_list (exclude the header row)\n",
    "    current_ticker_list = [j[0].strip() for j in  current_companies_list[1:]]\n",
    "    for i in company_changes_list:\n",
    "        # if input date is more recent than first change, return current list \n",
    "        if input_date_dt > i[0]: \n",
    "            break\n",
    "        elif input_date_dt <= i[0]:\n",
    "            if i[1] and i[1] in current_ticker_list: \n",
    "                current_ticker_list.remove(i[1])\n",
    "            if i[3]: \n",
    "                current_ticker_list.append(i[3])\n",
    "        else: \n",
    "            break\n",
    "    current_ticker_list.sort()\n",
    "    if output_type.strip() == 'string': \n",
    "        current_tickers = ','.join(current_ticker_list)\n",
    "    if output_type.strip() == 'list': \n",
    "        current_tickers = current_ticker_list \n",
    "    return current_tickers\n",
    "\n",
    "active_tickers = active_ticker_list_builder('2017-01-01', output_type='string')\n",
    "\n",
    "active_tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing thoughts  \n",
    "This is obviously a pretty specific use case, so hopefully you find some parts of it helpful. I wrote this as a stand-alone example but if this something you're doing regularly, it's probably better to store these tables in a database and run a daily script with a chron job or something to update the changes (that's my plan, at least). \n",
    "\n",
    "If you're only interested in the current list of companies, the first table usually behaves pretty well with the Pandas `read_html()` function, so that's an easy workaround if you don't care about all the historical changes. \n",
    "\n",
    "Drop me a line if you see any bugs/errors or if you have a better way to do this. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nikola": {
   "date": "2019-01-18 19:52:05 UTC",
   "slug": "Scraping-SP500-companies-wikipedia",
   "tags": "python, web scraping, stock market data",
   "title": "Scraping S&P 500 Components from Wikipedia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
